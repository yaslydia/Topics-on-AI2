{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlptSfqIT-fL"
   },
   "source": [
    "# CPSC 532S Assigment 5(b): Graph Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MFUjAT-T-fN"
   },
   "source": [
    "The goal of this part of the assignment is to get you familiar with implementing graph neural network in pytorch. Recently, there has been an explosion of research in the field of graph neural network. In this assignment we will start with the vanilla variant of graph neural network and develop other variants such as gated GNN and Graph Attention Networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM-S-b0RT-fN"
   },
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqOrL2DYT-fO"
   },
   "source": [
    "You will need to install pytroch_geometric for this part of the assigment.\n",
    "\n",
    "\"*PyTorch Geometric (PyG) is a geometric deep learning extension library for PyTorch.\n",
    "It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers. In addition, it consists of an easy-to-use mini-batch loader for many small and single giant graphs, multi gpu-support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds*\"\n",
    "\n",
    "To install pytorch geometric follow the instructions below:\n",
    "\n",
    "1. Determine the pytorch version and CUA version pytroch was installed with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n1zDJdY_T-fO",
    "outputId": "9409f347-774c-4f74-d508-cf02437d72f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 1.12.1\n",
      "CUDA version: 11.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "print('Pytorch Version:',torch.__version__); print('CUDA version:',torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_qCa6aaT-fP"
   },
   "source": [
    "Now we will set some environment variable that will indicate which version of the pytorch gemetric binaries(and dependencies) we will have to install. \n",
    "\n",
    "You will need a pytroch with version >= 1.4.0. You will have to set the `$TORCH` variable to one of(`1.4.0`, `1.5.0`, `1.6.0`, `1.7.0`, `1.8.0`). For a pytroch version of the form `1.x.y` set the `$TORCH` variable to `1.x.0` in the cell below. For example, I have pytroch `1.7.1` so i set the variable to `1.7.0`. \n",
    "\n",
    "Similarly of the cuda versions, set the `$CUDA` variable to on of (`cpu`, `cu92`, `cu101`, `cu102`, `cu110`, `cu111`). The number at the end indicate the cuda version. If you have a cuda version `10.1` then use `cu101`.\n",
    "\n",
    "**Make sure you set this appropiately or else the libraires wont work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5SPRqy9oT-fP"
   },
   "outputs": [],
   "source": [
    "os.environ['TORCH'] = '1.12.0'\n",
    "os.environ['CUDA'] = 'cu113'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PPVjLcsT-fP"
   },
   "source": [
    "Install the relevant packages using the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p9R7qZs0T-fQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.0+cu113.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.1.0%2Bpt112cu113-cp37-cp37m-linux_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.0+pt112cu113\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.0+cu113.html\n",
      "Collecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15%2Bpt112cu113-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse) (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse) (1.21.6)\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.6.15+pt112cu113\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.0+cu113.html\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0%2Bpt112cu113-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-cluster) (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-cluster) (1.21.6)\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.0+pt112cu113\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.0+cu113.html\n",
      "Collecting torch-spline-conv\n",
      "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1%2Bpt112cu113-cp37-cp37m-linux_x86_64.whl (722 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.7/722.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
      "Successfully installed torch-spline-conv-1.2.1+pt112cu113\n",
      "Collecting torch-geometric\n",
      "  Using cached torch_geometric-2.2.0.tar.gz (564 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.64.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.21.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (5.9.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
      "Building wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=854bd16b213474a30341e23bd3e2edc1283d1a4913a5af8bf0e174356020067a\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/83/b2/dc/5a92df31573f946e8f3ca62b861be8a6a55c5228ed27cc989b\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GavRs80CT-fQ"
   },
   "source": [
    "The main reason we are using Pytorch Geometric of this assignemt is beacuse it has a large collection of datasets that can be readily used. Additionally, they have implementation of Dataloader which can be tricky in case of graph as batching graphs with varying number of nodes is not as easy as stacking in case of images.\n",
    "\n",
    "Please go through https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html to understand how Pytroch Geometric handles graph data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjATx0udT-fQ"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnSbMdTLT-fQ"
   },
   "source": [
    "In this assignment we will use Cora dataset.\n",
    "\n",
    "\"*The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.*\"\n",
    "\n",
    "The nodes in the graph are publications and edges correspond to citations: if publication A cites publication B, then the graph has an edge from A to B. The task here is node classification where we want to classify each paper into one of seven classes :\n",
    " - `Case_Based` \n",
    " - `Genetic_Algorithms`\n",
    " - `Neural_Networks`\n",
    " - `Probabilistic_Methods`\n",
    " - `Reinforcement_Learning`\n",
    " - `Rule_Learning`\n",
    " - `Theory`\n",
    " \n",
    " The dataset is readily available in pytorch geometric and we will use that to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YeF98yDkT-fR",
    "outputId": "33fd906c-884a-4a1d-f1ba-bf6dd84f6b12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in the dataset   : 1\n",
      "Number of nodes the entire graph  : 2708\n",
      "Numver of nodes in the train set  : 140\n",
      "Numver of nodes in the val   set  : 500\n",
      "Numver of nodes in the test  set  : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "path = 'datasets/'\n",
    "dataset = Planetoid(root=path, name='Cora')\n",
    "print('Number of graphs in the dataset   :',len(dataset))\n",
    "print('Number of nodes the entire graph  :', dataset[0].x.shape[0])\n",
    "print('Numver of nodes in the train set  :', dataset[0].train_mask.sum().item())\n",
    "print('Numver of nodes in the val   set  :', dataset[0].val_mask.sum().item())\n",
    "print('Numver of nodes in the test  set  :', dataset[0].test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMLg2otMT-fR"
   },
   "source": [
    "**NOTE:** The dataset one contains one giant graph. The dataset is split into train, val and test in terms of the nodes i.e. we will use the the labels of some of the nodes (specified by train mask) to train our network and we will test by classifying the nodes in the test set. This is slightly different from what you are generally in case of image or text data. Also bear in mind that not all graph datasets will in the form of one giant graph. For example when working with molecular dataset each graph will correspond to a chemical compound and your dataset will have a collection of disjoint graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vWS6EcROT-fR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch import Tensor\n",
    "from torch_scatter import gather_csr, scatter\n",
    "from torch_geometric.utils import add_self_loops, degree, softmax\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng8mCxAST-fR"
   },
   "source": [
    "## Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKwom815T-fS"
   },
   "source": [
    "We will first set up a base class that will act as a parent for subsequent convolution layers that you will implement. Please go through the base class to understand what each method in the class is used for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9qRLETXBT-fS"
   },
   "outputs": [],
   "source": [
    "class MessagePassing(nn.Module):\n",
    "    r\"\"\"\n",
    "    Base Class for implementing Message Passing Layers.\n",
    "    A message passing layer has three main components:\n",
    "        1. Message    : Specifies what infromation should \n",
    "                        be passed out from a node.\n",
    "        2. Aggreagate : Specifies how infromation will be \n",
    "                        propogated to neighbours.\n",
    "        3. Combine    : Specifies how the incoming message \n",
    "                        will be used to update the current state.\n",
    "    Note: We will only focus on graph with features only on nodes\n",
    "          in this assignment. It should give you a good idea\n",
    "          of how to implement message passing layer and how to\n",
    "          extend it to graphs with edge features.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, node_dim: int = -2, aggr: str = \"add\"):\n",
    "            super().__init__()\n",
    "            self.node_dim = node_dim\n",
    "            self.aggr = aggr\n",
    "    \n",
    "    def message(self, node_states: Tensor, edge_index) -> Tensor:\n",
    "        r\"\"\"\n",
    "        From the nodes states contruct the message the\n",
    "        goes out to all the neighbours.\n",
    "        This step usually involves passing the nodes\n",
    "        states though some linear layer. Once transformed\n",
    "        you will have to transfrom the states to messages\n",
    "        over the edges.\n",
    "        Args:\n",
    "        -----\n",
    "            nodes_states: A tensor containing the states of\n",
    "                          nodes in a graph\n",
    "            edge_index: A 2xE tensor specifying the connectivity \n",
    "                        structure of a graph.\n",
    "                        Ex: For the graph below\n",
    "                                  0\n",
    "                                /   \\\n",
    "                               1     2\n",
    "                            edge_index = [\n",
    "                                            [0, 0],\n",
    "                                            [1, 2]\n",
    "                            ]\n",
    "                            The first colum specifies the edge\n",
    "                            (0,1) and the second column specifies\n",
    "                            the edge (0,2).\n",
    "                        The matrix is specified this was rather \n",
    "                        than a nxn adjacency matrix with 0 and 1\n",
    "                        entries for memory and computational \n",
    "                        efficiency. Adjacency matrices are\n",
    "                        often quite sparse and represting them\n",
    "                        as dense matrices can lead to waste of\n",
    "                        memory and compute time.\n",
    "        Returns:\n",
    "        --------\n",
    "            A tensor that transforms the nodes states to the\n",
    "            required message\n",
    "        \"\"\"\n",
    "        #For the base class we will just return the nodes \n",
    "        #states as is.\n",
    "        return node_states[edge_index[0]]\n",
    "    \n",
    "    def aggregate(self, messages: Tensor = None, edge_index: Tensor = None, dim_size = None) -> Tensor:\n",
    "        r\"\"\"\n",
    "        For each node aggregate the messages from its neighbours\n",
    "        Args:\n",
    "        -----\n",
    "            messages  : The transformed nodes staes\n",
    "            edge_index: A 2xE tensor specifying the connectivity \n",
    "                        structure of a graph.\n",
    "            dim_size  : Number of nodes in the graph\n",
    "        Returns:\n",
    "        -------\n",
    "            A tensor corresponding to aggregates messages\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def combine(self, node_states: Tensor, agg_message:Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        A function to combine the incomming message and the current\n",
    "        state of the nodes.\n",
    "        Args:\n",
    "        ----\n",
    "            nodes_states: The current states of the nodes\n",
    "            agg_message : Messages aggregated from its neighbours\n",
    "        Returns:\n",
    "        --------\n",
    "            Updates nodes states\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RGuwTnpmT-fS"
   },
   "outputs": [],
   "source": [
    "class GraphConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GraphConv, self).__init__()\n",
    "        \n",
    "        self.kernel = nn.Linear(in_channels, out_channels)\n",
    "    \n",
    "    def messages(self, node_states, edge_index, edge_weights):\n",
    "        '''\n",
    "        Function to convert node states to message over the edges\n",
    "        Args:\n",
    "        ----\n",
    "            node_states  : Node features\n",
    "            edge_index   : 2xE tensor specifying graph structure\n",
    "            edge_weights : Weights to normalize the messages.\n",
    "        '''\n",
    "        \n",
    "        messages = None\n",
    "        # Your code goes here (note that this can be implemented in as little as 3 lines of code)\n",
    "        messages = self.kernel(node_states)\n",
    "        messages = messages[edge_index[0]]\n",
    "        # print(messages.shape, edge_weights.shape)\n",
    "        messages = messages * edge_weights.unsqueeze(-1)\n",
    "\n",
    "        #####################\n",
    "        return messages\n",
    "    \n",
    "    def aggregate(self, messages, edge_index, dim_size):\n",
    "        '''\n",
    "        Function to convert node states to message over the edges\n",
    "        Args:\n",
    "        ----\n",
    "            node_states  : Node features\n",
    "            edge_index   : 2xE tensor specifying graph structure\n",
    "            edge_weights : Weights to normalize the messages.\n",
    "        '''\n",
    "        \n",
    "        # Your code goes here (note that this can be implemented in 1-2 lines of code)\n",
    "        #Hint: You will have to use the scatter function from torch_scatter\n",
    "        out = scatter(messages, edge_index[1], dim=0, dim_size=dim_size, reduce='mean')\n",
    "\n",
    "        #####################\n",
    "        return out\n",
    "        \n",
    "    def combine(self, old_states, new_states):\n",
    "        return new_states\n",
    "    \n",
    "    def compute_norm(self, edge_index, dim_size):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, dim_size, dtype=edge_index.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return norm\n",
    "    \n",
    "    def forward(self, node_states, edge_index):\n",
    "        #Add self loops into the graph\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=node_states.size(0))\n",
    "        #Compute number of nodes in the graph\n",
    "        dim_size = node_states.size(self.node_dim)\n",
    "        #Compute the normalization constants\n",
    "        norm = self.compute_norm(edge_index, dim_size)\n",
    "        #Create the messages from the node states\n",
    "        messages = self.messages(node_states.clone(), edge_index, norm)\n",
    "        #Aggregate the node states\n",
    "        new_states = self.aggregate(messages, edge_index, dim_size)\n",
    "        #Combine the new states with the old one\n",
    "        return self.combine(node_states, new_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykUhuz5nT-fS"
   },
   "source": [
    "Now let implement the Graph Convolution Network (GCN) as proposed in [Semi-Supervised Classification with Graph Convolutional \n",
    "Networks](https://arxiv.org/abs/1609.02907).\n",
    "\n",
    "The convolution operation in a GCN is given by\n",
    "$$Z = \\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2} W X$$\n",
    "where $$\\hat{A} = A + I$$ is the adjaceny matrix augmented with self loops and D is a diagonal degree matrix with diagonal entries(D_ii) equal to number of edge outgoing from node i. W is the kernel matrix that transform the node states X.\n",
    "\n",
    "We have defined the `GraphConv` layer that inherits `MessagePassing` below. You have to implement `message` and `aggregate` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGlKqKuWT-fT"
   },
   "source": [
    "We will define a graph convolution network using the network you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UFsJUH1dT-fT"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GraphConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "A36KGkNkT-fT"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fZVGk28bT-fU"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mpu1x5yET-fU"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4OtlG8XMT-fU",
    "outputId": "0cd46b8a-7dbb-48d0-d286-dbeaccd9dba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.1429, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 002, Train: 0.1500, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 003, Train: 0.1929, Val: 0.3160, Test: 0.3190\n",
      "Epoch: 004, Train: 0.2857, Val: 0.3260, Test: 0.3330\n",
      "Epoch: 005, Train: 0.3286, Val: 0.3380, Test: 0.3510\n",
      "Epoch: 006, Train: 0.3571, Val: 0.3480, Test: 0.3650\n",
      "Epoch: 007, Train: 0.4143, Val: 0.3740, Test: 0.3870\n",
      "Epoch: 008, Train: 0.4643, Val: 0.3960, Test: 0.4100\n",
      "Epoch: 009, Train: 0.4929, Val: 0.4000, Test: 0.4180\n",
      "Epoch: 010, Train: 0.5357, Val: 0.4000, Test: 0.4180\n",
      "Epoch: 011, Train: 0.5500, Val: 0.4060, Test: 0.4190\n",
      "Epoch: 012, Train: 0.5857, Val: 0.4060, Test: 0.4190\n",
      "Epoch: 013, Train: 0.6071, Val: 0.4060, Test: 0.4190\n",
      "Epoch: 014, Train: 0.6286, Val: 0.4200, Test: 0.4380\n",
      "Epoch: 015, Train: 0.6500, Val: 0.4300, Test: 0.4470\n",
      "Epoch: 016, Train: 0.6571, Val: 0.4340, Test: 0.4470\n",
      "Epoch: 017, Train: 0.6929, Val: 0.4480, Test: 0.4510\n",
      "Epoch: 018, Train: 0.7214, Val: 0.4640, Test: 0.4640\n",
      "Epoch: 019, Train: 0.7429, Val: 0.4640, Test: 0.4640\n",
      "Epoch: 020, Train: 0.7500, Val: 0.4700, Test: 0.4930\n",
      "Epoch: 021, Train: 0.7643, Val: 0.4880, Test: 0.5100\n",
      "Epoch: 022, Train: 0.7643, Val: 0.4980, Test: 0.5190\n",
      "Epoch: 023, Train: 0.7786, Val: 0.5020, Test: 0.5280\n",
      "Epoch: 024, Train: 0.7786, Val: 0.5120, Test: 0.5340\n",
      "Epoch: 025, Train: 0.7786, Val: 0.5160, Test: 0.5450\n",
      "Epoch: 026, Train: 0.7786, Val: 0.5240, Test: 0.5500\n",
      "Epoch: 027, Train: 0.7786, Val: 0.5260, Test: 0.5550\n",
      "Epoch: 028, Train: 0.7857, Val: 0.5380, Test: 0.5640\n",
      "Epoch: 029, Train: 0.7929, Val: 0.5460, Test: 0.5680\n",
      "Epoch: 030, Train: 0.8000, Val: 0.5540, Test: 0.5740\n",
      "Epoch: 031, Train: 0.8071, Val: 0.5680, Test: 0.5790\n",
      "Epoch: 032, Train: 0.8143, Val: 0.5780, Test: 0.5840\n",
      "Epoch: 033, Train: 0.8286, Val: 0.5820, Test: 0.5910\n",
      "Epoch: 034, Train: 0.8357, Val: 0.5880, Test: 0.5990\n",
      "Epoch: 035, Train: 0.8571, Val: 0.5980, Test: 0.6160\n",
      "Epoch: 036, Train: 0.8571, Val: 0.6080, Test: 0.6230\n",
      "Epoch: 037, Train: 0.8571, Val: 0.6160, Test: 0.6240\n",
      "Epoch: 038, Train: 0.8643, Val: 0.6200, Test: 0.6290\n",
      "Epoch: 039, Train: 0.8643, Val: 0.6200, Test: 0.6290\n",
      "Epoch: 040, Train: 0.8643, Val: 0.6220, Test: 0.6420\n",
      "Epoch: 041, Train: 0.8643, Val: 0.6300, Test: 0.6490\n",
      "Epoch: 042, Train: 0.8714, Val: 0.6300, Test: 0.6490\n",
      "Epoch: 043, Train: 0.8714, Val: 0.6400, Test: 0.6580\n",
      "Epoch: 044, Train: 0.8643, Val: 0.6500, Test: 0.6620\n",
      "Epoch: 045, Train: 0.8786, Val: 0.6540, Test: 0.6650\n",
      "Epoch: 046, Train: 0.8786, Val: 0.6560, Test: 0.6720\n",
      "Epoch: 047, Train: 0.8857, Val: 0.6660, Test: 0.6800\n",
      "Epoch: 048, Train: 0.9000, Val: 0.6740, Test: 0.6920\n",
      "Epoch: 049, Train: 0.9143, Val: 0.6800, Test: 0.7040\n",
      "Epoch: 050, Train: 0.9143, Val: 0.6840, Test: 0.7110\n",
      "Epoch: 051, Train: 0.9071, Val: 0.6880, Test: 0.7120\n",
      "Epoch: 052, Train: 0.9214, Val: 0.6900, Test: 0.7190\n",
      "Epoch: 053, Train: 0.9357, Val: 0.6960, Test: 0.7250\n",
      "Epoch: 054, Train: 0.9357, Val: 0.6980, Test: 0.7280\n",
      "Epoch: 055, Train: 0.9357, Val: 0.7040, Test: 0.7350\n",
      "Epoch: 056, Train: 0.9357, Val: 0.7140, Test: 0.7370\n",
      "Epoch: 057, Train: 0.9429, Val: 0.7160, Test: 0.7390\n",
      "Epoch: 058, Train: 0.9357, Val: 0.7200, Test: 0.7410\n",
      "Epoch: 059, Train: 0.9357, Val: 0.7220, Test: 0.7450\n",
      "Epoch: 060, Train: 0.9357, Val: 0.7260, Test: 0.7470\n",
      "Epoch: 061, Train: 0.9500, Val: 0.7300, Test: 0.7510\n",
      "Epoch: 062, Train: 0.9500, Val: 0.7320, Test: 0.7540\n",
      "Epoch: 063, Train: 0.9571, Val: 0.7340, Test: 0.7600\n",
      "Epoch: 064, Train: 0.9571, Val: 0.7400, Test: 0.7620\n",
      "Epoch: 065, Train: 0.9643, Val: 0.7460, Test: 0.7620\n",
      "Epoch: 066, Train: 0.9643, Val: 0.7460, Test: 0.7620\n",
      "Epoch: 067, Train: 0.9714, Val: 0.7480, Test: 0.7680\n",
      "Epoch: 068, Train: 0.9714, Val: 0.7480, Test: 0.7680\n",
      "Epoch: 069, Train: 0.9714, Val: 0.7480, Test: 0.7680\n",
      "Epoch: 070, Train: 0.9714, Val: 0.7500, Test: 0.7700\n",
      "Epoch: 071, Train: 0.9714, Val: 0.7500, Test: 0.7700\n",
      "Epoch: 072, Train: 0.9714, Val: 0.7500, Test: 0.7700\n",
      "Epoch: 073, Train: 0.9714, Val: 0.7520, Test: 0.7740\n",
      "Epoch: 074, Train: 0.9714, Val: 0.7520, Test: 0.7740\n",
      "Epoch: 075, Train: 0.9714, Val: 0.7540, Test: 0.7770\n",
      "Epoch: 076, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 077, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 078, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 079, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 080, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 081, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 082, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 083, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 084, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 085, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 086, Train: 0.9714, Val: 0.7580, Test: 0.7760\n",
      "Epoch: 087, Train: 0.9714, Val: 0.7600, Test: 0.7930\n",
      "Epoch: 088, Train: 0.9714, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 089, Train: 0.9714, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 090, Train: 0.9714, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 091, Train: 0.9857, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 092, Train: 0.9857, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 093, Train: 0.9857, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 094, Train: 0.9857, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 095, Train: 0.9857, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 096, Train: 0.9857, Val: 0.7620, Test: 0.7930\n",
      "Epoch: 097, Train: 0.9857, Val: 0.7640, Test: 0.7940\n",
      "Epoch: 098, Train: 0.9857, Val: 0.7660, Test: 0.7950\n",
      "Epoch: 099, Train: 0.9857, Val: 0.7680, Test: 0.7950\n",
      "Epoch: 100, Train: 0.9857, Val: 0.7680, Test: 0.7950\n",
      "Epoch: 101, Train: 0.9857, Val: 0.7680, Test: 0.7950\n",
      "Epoch: 102, Train: 0.9857, Val: 0.7700, Test: 0.7960\n",
      "Epoch: 103, Train: 0.9857, Val: 0.7700, Test: 0.7960\n",
      "Epoch: 104, Train: 0.9857, Val: 0.7700, Test: 0.7960\n",
      "Epoch: 105, Train: 0.9929, Val: 0.7720, Test: 0.7990\n",
      "Epoch: 106, Train: 0.9929, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 107, Train: 0.9929, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 108, Train: 0.9929, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 109, Train: 0.9929, Val: 0.7740, Test: 0.7990\n",
      "Epoch: 110, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 111, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 112, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 113, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 114, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 115, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 116, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 117, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 118, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 119, Train: 0.9929, Val: 0.7760, Test: 0.7960\n",
      "Epoch: 120, Train: 0.9929, Val: 0.7780, Test: 0.7950\n",
      "Epoch: 121, Train: 0.9929, Val: 0.7780, Test: 0.7950\n",
      "Epoch: 122, Train: 0.9929, Val: 0.7780, Test: 0.7950\n",
      "Epoch: 123, Train: 0.9929, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 124, Train: 0.9929, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 125, Train: 0.9929, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 126, Train: 0.9929, Val: 0.7800, Test: 0.7960\n",
      "Epoch: 127, Train: 0.9929, Val: 0.7820, Test: 0.7980\n",
      "Epoch: 128, Train: 0.9929, Val: 0.7840, Test: 0.7980\n",
      "Epoch: 129, Train: 0.9929, Val: 0.7840, Test: 0.7980\n",
      "Epoch: 130, Train: 0.9929, Val: 0.7860, Test: 0.7980\n",
      "Epoch: 131, Train: 0.9929, Val: 0.7860, Test: 0.7980\n",
      "Epoch: 132, Train: 0.9929, Val: 0.7860, Test: 0.7980\n",
      "Epoch: 133, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 134, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 135, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 136, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 137, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 138, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 139, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 140, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 141, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 142, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 143, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 144, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 145, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 146, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 147, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 148, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 149, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 150, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 151, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 152, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 153, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 154, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 155, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 156, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 157, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 158, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 159, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 160, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 161, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 162, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 163, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 164, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 165, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 166, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 167, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 168, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 169, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 170, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 171, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 172, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 173, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 174, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 175, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 176, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 177, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 178, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 179, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 180, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 181, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 182, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 183, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 184, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 185, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 186, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 187, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 188, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 189, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 190, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 191, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 192, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 193, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 194, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 195, Train: 0.9857, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 196, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 197, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 198, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 199, Train: 0.9929, Val: 0.7880, Test: 0.7970\n",
      "Epoch: 200, Train: 0.9929, Val: 0.7880, Test: 0.7970\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_acc, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx51pSpNT-fV"
   },
   "source": [
    "**The expected test accuracy is > 75%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ubqN1ieT-fV"
   },
   "source": [
    "## Graph Attention Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1LQgJppT-fV"
   },
   "source": [
    "Next we will implement the [Graph Attention Network](https://arxiv.org/abs/1710.10903) introduced in this [paper](https://arxiv.org/abs/1710.10903). We will start by defining additional initialization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6i9rYuh7T-fV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GaH2gnlT-fV"
   },
   "source": [
    "You will have to fill in the `messages` and `aggregate` method for Graph Attention Network below. Note: We will define a graph attention Layer with a single head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Vev5bLCxT-fW"
   },
   "outputs": [],
   "source": [
    "class GraphAttention(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, \n",
    "                 out_channels,\n",
    "                 negative_slope=0.2,\n",
    "                 dropout=0, bias=True\n",
    "                ):\n",
    "        \n",
    "        super(GraphAttention, self).__init__(node_dim=0)\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.kernel = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        \n",
    "        self.att_l = nn.Parameter(torch.Tensor(1, out_channels))\n",
    "        self.att_r = nn.Parameter(torch.Tensor(1, out_channels))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        glorot(self.kernel.weight)\n",
    "        glorot(self.att_l)\n",
    "        glorot(self.att_r)\n",
    "        zeros(self.bias)\n",
    "    \n",
    "    def messages(self, node_states, edge_index):\n",
    "        '''\n",
    "        Function to convert node states to message over the edges\n",
    "        Args:\n",
    "        ----\n",
    "            node_states  : Node features\n",
    "            edge_index   : 2xE tensor specifying graph structure\n",
    "        '''\n",
    "        C = self.out_channels\n",
    "        num_nodes = node_states.shape[0]\n",
    "        \n",
    "        node_states = self.kernel(node_states).view(-1, C)\n",
    "        \n",
    "        alpha_l = torch.matmul(node_states, self.att_l.T) # Your code\n",
    "        alpha_r = torch.matmul(node_states, self.att_r.T) # Your code\n",
    "        \n",
    "        # Compute messages \n",
    "        messages = node_states[edge_index[0]]\n",
    "        # Compute edge attention\n",
    "        alpha = nn.LeakyReLU(self.negative_slope)(alpha_l + alpha_r.T) # Your code for unnormalized attention\n",
    "        alpha = alpha[edge_index[0], edge_index[1]]\n",
    "        index = edge_index[1] # Destination index of the node for messages\n",
    "\n",
    "        alpha = softmax(alpha, index, None, num_nodes)\n",
    "        self._alpha = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return messages * alpha.unsqueeze(-1)\n",
    "    \n",
    "    def aggregate(self, messages, edge_index, dim_size):\n",
    "        '''\n",
    "        Function to convert node states to message over the edges\n",
    "        Args:\n",
    "        ----\n",
    "            node_states  : Node features\n",
    "            edge_index   : 2xE tensor specifying graph structure\n",
    "            edge_weights : Weights to normalize the messages.\n",
    "        '''   \n",
    "        # Your code goes here (note that this can be implemented in 1-2 lines of code)\n",
    "        # Hint: You will have to use the scatter function from torch_scatter\n",
    "        # This is identical to the standard GNN above (you can simply copy it over)\n",
    "\n",
    "        out = scatter(messages, edge_index[1], dim=0, dim_size=dim_size)\n",
    "        #####################\n",
    "        return out\n",
    "\n",
    "    def combine(self, old_states, new_states):\n",
    "        return new_states\n",
    "    \n",
    "    def forward(self, node_states, edge_index):       \n",
    "        #Add self loops into the graph\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=node_states.size(0))\n",
    "        #Compute number of nodes in the graph\n",
    "        dim_size = node_states.size(self.node_dim)\n",
    "        \n",
    "        #Create the messages from the node states\n",
    "        messages = self.messages(node_states.clone(), edge_index)\n",
    "        #Aggregate the node states\n",
    "        new_states = self.aggregate(messages, edge_index, dim_size)\n",
    "        new_states = new_states.view(-1, self.out_channels)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            new_states += self.bias\n",
    "            \n",
    "        #Combine the new states with the old one\n",
    "        return self.combine(node_states, new_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "NwUcSCwgT-fW"
   },
   "outputs": [],
   "source": [
    "dataset = Planetoid('datasets/', 'Cora', transform=T.NormalizeFeatures())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "UA5dqoV7T-fW"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GraphAttention(dataset.num_features, 64, dropout=0.6)\n",
    "        # On the Pubmed dataset, use heads=8 in conv2.\n",
    "        self.conv2 = GraphAttention(8 * 8, dataset.num_classes, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = F.dropout(data.x, p=0.6, training=self.training)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        x = F.elu(self.conv1(x, data.edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "BnJZgGFuT-fW"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "yEAZAu8GT-fW"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "MsWoC6eOT-fX",
    "outputId": "2f083142-ea8d-4102-e6de-7068cee9b626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.1429, Val: 0.1220, Test: 0.1130\n",
      "Epoch: 002, Train: 0.1571, Val: 0.1220, Test: 0.1120\n",
      "Epoch: 003, Train: 0.2643, Val: 0.1560, Test: 0.1460\n",
      "Epoch: 004, Train: 0.5214, Val: 0.3000, Test: 0.3120\n",
      "Epoch: 005, Train: 0.7929, Val: 0.4740, Test: 0.5000\n",
      "Epoch: 006, Train: 0.8786, Val: 0.6380, Test: 0.6550\n",
      "Epoch: 007, Train: 0.9429, Val: 0.7180, Test: 0.7130\n",
      "Epoch: 008, Train: 0.9357, Val: 0.7180, Test: 0.7450\n",
      "Epoch: 009, Train: 0.9357, Val: 0.7160, Test: 0.7460\n",
      "Epoch: 010, Train: 0.9357, Val: 0.7360, Test: 0.7560\n",
      "Epoch: 011, Train: 0.9214, Val: 0.7360, Test: 0.7540\n",
      "Epoch: 012, Train: 0.9286, Val: 0.7420, Test: 0.7500\n",
      "Epoch: 013, Train: 0.9286, Val: 0.7340, Test: 0.7510\n",
      "Epoch: 014, Train: 0.9286, Val: 0.7360, Test: 0.7530\n",
      "Epoch: 015, Train: 0.9286, Val: 0.7420, Test: 0.7630\n",
      "Epoch: 016, Train: 0.9357, Val: 0.7540, Test: 0.7700\n",
      "Epoch: 017, Train: 0.9286, Val: 0.7560, Test: 0.7720\n",
      "Epoch: 018, Train: 0.9357, Val: 0.7600, Test: 0.7620\n",
      "Epoch: 019, Train: 0.9500, Val: 0.7520, Test: 0.7450\n",
      "Epoch: 020, Train: 0.9286, Val: 0.7360, Test: 0.7370\n",
      "Epoch: 021, Train: 0.9286, Val: 0.7420, Test: 0.7340\n",
      "Epoch: 022, Train: 0.9214, Val: 0.7260, Test: 0.7270\n",
      "Epoch: 023, Train: 0.9357, Val: 0.7180, Test: 0.7210\n",
      "Epoch: 024, Train: 0.9286, Val: 0.7100, Test: 0.7170\n",
      "Epoch: 025, Train: 0.9286, Val: 0.7180, Test: 0.7220\n",
      "Epoch: 026, Train: 0.9357, Val: 0.7380, Test: 0.7340\n",
      "Epoch: 027, Train: 0.9429, Val: 0.7460, Test: 0.7340\n",
      "Epoch: 028, Train: 0.9500, Val: 0.7400, Test: 0.7390\n",
      "Epoch: 029, Train: 0.9429, Val: 0.7400, Test: 0.7440\n",
      "Epoch: 030, Train: 0.9500, Val: 0.7540, Test: 0.7540\n",
      "Epoch: 031, Train: 0.9643, Val: 0.7680, Test: 0.7620\n",
      "Epoch: 032, Train: 0.9643, Val: 0.7720, Test: 0.7680\n",
      "Epoch: 033, Train: 0.9571, Val: 0.7800, Test: 0.7700\n",
      "Epoch: 034, Train: 0.9571, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 035, Train: 0.9571, Val: 0.7840, Test: 0.7830\n",
      "Epoch: 036, Train: 0.9643, Val: 0.7920, Test: 0.7880\n",
      "Epoch: 037, Train: 0.9714, Val: 0.7940, Test: 0.7920\n",
      "Epoch: 038, Train: 0.9714, Val: 0.7920, Test: 0.7910\n",
      "Epoch: 039, Train: 0.9714, Val: 0.7940, Test: 0.7900\n",
      "Epoch: 040, Train: 0.9643, Val: 0.7900, Test: 0.7900\n",
      "Epoch: 041, Train: 0.9643, Val: 0.7880, Test: 0.7860\n",
      "Epoch: 042, Train: 0.9643, Val: 0.7860, Test: 0.7880\n",
      "Epoch: 043, Train: 0.9643, Val: 0.7840, Test: 0.7880\n",
      "Epoch: 044, Train: 0.9643, Val: 0.7840, Test: 0.7840\n",
      "Epoch: 045, Train: 0.9643, Val: 0.7840, Test: 0.7960\n",
      "Epoch: 046, Train: 0.9714, Val: 0.7880, Test: 0.8010\n",
      "Epoch: 047, Train: 0.9571, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 048, Train: 0.9571, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 049, Train: 0.9643, Val: 0.8080, Test: 0.8240\n",
      "Epoch: 050, Train: 0.9643, Val: 0.8040, Test: 0.8260\n",
      "Epoch: 051, Train: 0.9643, Val: 0.8040, Test: 0.8260\n",
      "Epoch: 052, Train: 0.9643, Val: 0.8040, Test: 0.8240\n",
      "Epoch: 053, Train: 0.9643, Val: 0.7960, Test: 0.8250\n",
      "Epoch: 054, Train: 0.9643, Val: 0.7960, Test: 0.8190\n",
      "Epoch: 055, Train: 0.9714, Val: 0.7900, Test: 0.8210\n",
      "Epoch: 056, Train: 0.9714, Val: 0.7880, Test: 0.8210\n",
      "Epoch: 057, Train: 0.9786, Val: 0.7860, Test: 0.8190\n",
      "Epoch: 058, Train: 0.9786, Val: 0.7860, Test: 0.8140\n",
      "Epoch: 059, Train: 0.9786, Val: 0.7860, Test: 0.8110\n",
      "Epoch: 060, Train: 0.9714, Val: 0.7900, Test: 0.8080\n",
      "Epoch: 061, Train: 0.9714, Val: 0.7880, Test: 0.8080\n",
      "Epoch: 062, Train: 0.9714, Val: 0.7920, Test: 0.8150\n",
      "Epoch: 063, Train: 0.9714, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 064, Train: 0.9714, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 065, Train: 0.9714, Val: 0.7940, Test: 0.8160\n",
      "Epoch: 066, Train: 0.9714, Val: 0.7940, Test: 0.8150\n",
      "Epoch: 067, Train: 0.9714, Val: 0.7960, Test: 0.8150\n",
      "Epoch: 068, Train: 0.9714, Val: 0.7920, Test: 0.8150\n",
      "Epoch: 069, Train: 0.9714, Val: 0.7860, Test: 0.8140\n",
      "Epoch: 070, Train: 0.9643, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 071, Train: 0.9643, Val: 0.7920, Test: 0.8040\n",
      "Epoch: 072, Train: 0.9714, Val: 0.7900, Test: 0.8020\n",
      "Epoch: 073, Train: 0.9786, Val: 0.7840, Test: 0.8000\n",
      "Epoch: 074, Train: 0.9857, Val: 0.7820, Test: 0.7960\n",
      "Epoch: 075, Train: 0.9857, Val: 0.7820, Test: 0.7990\n",
      "Epoch: 076, Train: 0.9857, Val: 0.7800, Test: 0.8010\n",
      "Epoch: 077, Train: 0.9857, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 078, Train: 0.9857, Val: 0.7840, Test: 0.8100\n",
      "Epoch: 079, Train: 0.9786, Val: 0.7820, Test: 0.8120\n",
      "Epoch: 080, Train: 0.9786, Val: 0.7740, Test: 0.8130\n",
      "Epoch: 081, Train: 0.9786, Val: 0.7720, Test: 0.8130\n",
      "Epoch: 082, Train: 0.9786, Val: 0.7700, Test: 0.8110\n",
      "Epoch: 083, Train: 0.9714, Val: 0.7700, Test: 0.8080\n",
      "Epoch: 084, Train: 0.9643, Val: 0.7740, Test: 0.8060\n",
      "Epoch: 085, Train: 0.9643, Val: 0.7720, Test: 0.8080\n",
      "Epoch: 086, Train: 0.9643, Val: 0.7740, Test: 0.8030\n",
      "Epoch: 087, Train: 0.9643, Val: 0.7740, Test: 0.8050\n",
      "Epoch: 088, Train: 0.9643, Val: 0.7740, Test: 0.8040\n",
      "Epoch: 089, Train: 0.9714, Val: 0.7780, Test: 0.8090\n",
      "Epoch: 090, Train: 0.9714, Val: 0.7800, Test: 0.8100\n",
      "Epoch: 091, Train: 0.9786, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 092, Train: 0.9786, Val: 0.7900, Test: 0.8100\n",
      "Epoch: 093, Train: 0.9786, Val: 0.7900, Test: 0.8130\n",
      "Epoch: 094, Train: 0.9786, Val: 0.7920, Test: 0.8180\n",
      "Epoch: 095, Train: 0.9857, Val: 0.8020, Test: 0.8230\n",
      "Epoch: 096, Train: 0.9857, Val: 0.8000, Test: 0.8210\n",
      "Epoch: 097, Train: 0.9857, Val: 0.8040, Test: 0.8250\n",
      "Epoch: 098, Train: 0.9857, Val: 0.8040, Test: 0.8270\n",
      "Epoch: 099, Train: 0.9857, Val: 0.8020, Test: 0.8260\n",
      "Epoch: 100, Train: 0.9857, Val: 0.8040, Test: 0.8290\n",
      "Epoch: 101, Train: 0.9857, Val: 0.8000, Test: 0.8290\n",
      "Epoch: 102, Train: 0.9857, Val: 0.7980, Test: 0.8270\n",
      "Epoch: 103, Train: 0.9857, Val: 0.7960, Test: 0.8280\n",
      "Epoch: 104, Train: 0.9857, Val: 0.7980, Test: 0.8250\n",
      "Epoch: 105, Train: 0.9857, Val: 0.7940, Test: 0.8210\n",
      "Epoch: 106, Train: 0.9857, Val: 0.7940, Test: 0.8190\n",
      "Epoch: 107, Train: 0.9857, Val: 0.7940, Test: 0.8200\n",
      "Epoch: 108, Train: 0.9857, Val: 0.7960, Test: 0.8230\n",
      "Epoch: 109, Train: 0.9857, Val: 0.7900, Test: 0.8240\n",
      "Epoch: 110, Train: 0.9714, Val: 0.7880, Test: 0.8220\n",
      "Epoch: 111, Train: 0.9714, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 112, Train: 0.9714, Val: 0.7860, Test: 0.8090\n",
      "Epoch: 113, Train: 0.9714, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 114, Train: 0.9714, Val: 0.7800, Test: 0.8040\n",
      "Epoch: 115, Train: 0.9786, Val: 0.7760, Test: 0.8030\n",
      "Epoch: 116, Train: 0.9786, Val: 0.7840, Test: 0.8010\n",
      "Epoch: 117, Train: 0.9786, Val: 0.7900, Test: 0.8070\n",
      "Epoch: 118, Train: 0.9786, Val: 0.7960, Test: 0.8120\n",
      "Epoch: 119, Train: 0.9786, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 120, Train: 0.9857, Val: 0.7980, Test: 0.8180\n",
      "Epoch: 121, Train: 0.9857, Val: 0.8000, Test: 0.8230\n",
      "Epoch: 122, Train: 0.9857, Val: 0.8060, Test: 0.8310\n",
      "Epoch: 123, Train: 0.9857, Val: 0.8020, Test: 0.8320\n",
      "Epoch: 124, Train: 0.9857, Val: 0.8080, Test: 0.8350\n",
      "Epoch: 125, Train: 0.9857, Val: 0.8120, Test: 0.8350\n",
      "Epoch: 126, Train: 0.9857, Val: 0.8080, Test: 0.8360\n",
      "Epoch: 127, Train: 0.9929, Val: 0.8060, Test: 0.8340\n",
      "Epoch: 128, Train: 0.9929, Val: 0.8060, Test: 0.8330\n",
      "Epoch: 129, Train: 0.9929, Val: 0.8020, Test: 0.8350\n",
      "Epoch: 130, Train: 0.9857, Val: 0.8000, Test: 0.8320\n",
      "Epoch: 131, Train: 0.9857, Val: 0.8020, Test: 0.8330\n",
      "Epoch: 132, Train: 0.9857, Val: 0.8020, Test: 0.8310\n",
      "Epoch: 133, Train: 0.9786, Val: 0.7920, Test: 0.8290\n",
      "Epoch: 134, Train: 0.9786, Val: 0.7920, Test: 0.8260\n",
      "Epoch: 135, Train: 0.9786, Val: 0.7880, Test: 0.8170\n",
      "Epoch: 136, Train: 0.9786, Val: 0.7860, Test: 0.8170\n",
      "Epoch: 137, Train: 0.9786, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 138, Train: 0.9786, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 139, Train: 0.9786, Val: 0.7860, Test: 0.8160\n",
      "Epoch: 140, Train: 0.9786, Val: 0.7940, Test: 0.8120\n",
      "Epoch: 141, Train: 0.9786, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 142, Train: 0.9786, Val: 0.7940, Test: 0.8140\n",
      "Epoch: 143, Train: 0.9786, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 144, Train: 0.9786, Val: 0.7900, Test: 0.8150\n",
      "Epoch: 145, Train: 0.9857, Val: 0.7900, Test: 0.8170\n",
      "Epoch: 146, Train: 0.9857, Val: 0.7900, Test: 0.8170\n",
      "Epoch: 147, Train: 0.9857, Val: 0.7920, Test: 0.8190\n",
      "Epoch: 148, Train: 0.9786, Val: 0.7940, Test: 0.8180\n",
      "Epoch: 149, Train: 0.9786, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 150, Train: 0.9786, Val: 0.7920, Test: 0.8150\n",
      "Epoch: 151, Train: 0.9786, Val: 0.7940, Test: 0.8170\n",
      "Epoch: 152, Train: 0.9714, Val: 0.7900, Test: 0.8150\n",
      "Epoch: 153, Train: 0.9714, Val: 0.7880, Test: 0.8150\n",
      "Epoch: 154, Train: 0.9714, Val: 0.7920, Test: 0.8180\n",
      "Epoch: 155, Train: 0.9714, Val: 0.7960, Test: 0.8190\n",
      "Epoch: 156, Train: 0.9786, Val: 0.7920, Test: 0.8190\n",
      "Epoch: 157, Train: 0.9786, Val: 0.7900, Test: 0.8200\n",
      "Epoch: 158, Train: 0.9786, Val: 0.7900, Test: 0.8150\n",
      "Epoch: 159, Train: 0.9786, Val: 0.7900, Test: 0.8160\n",
      "Epoch: 160, Train: 0.9857, Val: 0.7880, Test: 0.8140\n",
      "Epoch: 161, Train: 0.9857, Val: 0.7920, Test: 0.8140\n",
      "Epoch: 162, Train: 0.9857, Val: 0.7920, Test: 0.8170\n",
      "Epoch: 163, Train: 0.9857, Val: 0.7920, Test: 0.8170\n",
      "Epoch: 164, Train: 0.9857, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 165, Train: 0.9929, Val: 0.7960, Test: 0.8160\n",
      "Epoch: 166, Train: 0.9929, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 167, Train: 0.9929, Val: 0.8140, Test: 0.8140\n",
      "Epoch: 168, Train: 0.9929, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 169, Train: 0.9929, Val: 0.8120, Test: 0.8180\n",
      "Epoch: 170, Train: 0.9929, Val: 0.8140, Test: 0.8270\n",
      "Epoch: 171, Train: 1.0000, Val: 0.8140, Test: 0.8320\n",
      "Epoch: 172, Train: 1.0000, Val: 0.8120, Test: 0.8320\n",
      "Epoch: 173, Train: 1.0000, Val: 0.8120, Test: 0.8290\n",
      "Epoch: 174, Train: 1.0000, Val: 0.8100, Test: 0.8250\n",
      "Epoch: 175, Train: 1.0000, Val: 0.8120, Test: 0.8240\n",
      "Epoch: 176, Train: 1.0000, Val: 0.8100, Test: 0.8250\n",
      "Epoch: 177, Train: 1.0000, Val: 0.8060, Test: 0.8290\n",
      "Epoch: 178, Train: 1.0000, Val: 0.8060, Test: 0.8280\n",
      "Epoch: 179, Train: 1.0000, Val: 0.8080, Test: 0.8290\n",
      "Epoch: 180, Train: 1.0000, Val: 0.8100, Test: 0.8340\n",
      "Epoch: 181, Train: 0.9929, Val: 0.8080, Test: 0.8280\n",
      "Epoch: 182, Train: 0.9929, Val: 0.8120, Test: 0.8250\n",
      "Epoch: 183, Train: 0.9929, Val: 0.8100, Test: 0.8240\n",
      "Epoch: 184, Train: 0.9929, Val: 0.8140, Test: 0.8160\n",
      "Epoch: 185, Train: 0.9929, Val: 0.8160, Test: 0.8170\n",
      "Epoch: 186, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "Epoch: 187, Train: 0.9929, Val: 0.8060, Test: 0.8170\n",
      "Epoch: 188, Train: 0.9929, Val: 0.8080, Test: 0.8180\n",
      "Epoch: 189, Train: 0.9857, Val: 0.8040, Test: 0.8250\n",
      "Epoch: 190, Train: 0.9857, Val: 0.8060, Test: 0.8250\n",
      "Epoch: 191, Train: 0.9857, Val: 0.8040, Test: 0.8250\n",
      "Epoch: 192, Train: 0.9857, Val: 0.8020, Test: 0.8240\n",
      "Epoch: 193, Train: 0.9857, Val: 0.7980, Test: 0.8250\n",
      "Epoch: 194, Train: 0.9857, Val: 0.7960, Test: 0.8280\n",
      "Epoch: 195, Train: 0.9857, Val: 0.7960, Test: 0.8300\n",
      "Epoch: 196, Train: 0.9857, Val: 0.8000, Test: 0.8280\n",
      "Epoch: 197, Train: 0.9857, Val: 0.8000, Test: 0.8270\n",
      "Epoch: 198, Train: 0.9857, Val: 0.8000, Test: 0.8260\n",
      "Epoch: 199, Train: 0.9857, Val: 0.8020, Test: 0.8230\n",
      "Epoch: 200, Train: 0.9857, Val: 0.8000, Test: 0.8140\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, *test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CPSC532S_Assignment5_GNN.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
